{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Backpropagation y SGD: Desmitificando el Corazón de las Redes Neuronales\n",
                "\n",
                "¡Hola a todos! Es un placer acompañarlos en este camino de aprendizaje. El día de hoy, por encargo del **Dr. Fuentes**, exploraremos dos conceptos fundamentales que permiten que las máquinas \"aprendan\": el **Backpropagation** (Propagación hacia atrás) y el **SGD** (Descenso de Gradiente Estocástico).\n",
                "\n",
                "--- \n",
                "\n",
                "## 1. La Intuición Detrás del Aprendizaje\n",
                "\n",
                "Imaginen que están en la cima de una montaña neblinosa y quieren llegar al valle más bajo. No pueden ver el camino completo, pero pueden sentir la inclinación del suelo bajo sus pies. \n",
                "\n",
                "*   **La pendiente (Gradiente):** Nos dice hacia dónde subir. Si queremos bajar, debemos ir en dirección opuesta.\n",
                "*   **El paso (Learning Rate):** Qué tan grande es el paso que damos en esa dirección.\n",
                "\n",
                "En una red neuronal, la \"montaña\" es nuestra **Función de Pérdida** ($L$) y el suelo sobre el que caminamos está definido por los **Pesos** ($w$) y **Sesgos** ($b$) de la red."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Descenso de Gradiente Estocástico (SGD)\n",
                "\n",
                "El Descenso de Gradiente tradicional usa *todo* el conjunto de datos para calcular un solo paso. ¡Eso es muy lento si tenemos millones de datos! \n",
                "\n",
                "El **SGD** simplifica esto: en cada paso, toma **un solo ejemplo** (o un pequeño grupo llamado *mini-batch*) al azar. Aunque el camino sea un poco más \"ruidoso\", llega al objetivo mucho más rápido.\n",
                "\n",
                "### La ecuación matemática fundamental:\n",
                "$$\\theta_{t+1} = \\theta_t - \\eta \\cdot \\nabla_{\\theta} J(\\theta; x^{(i)}, y^{(i)})$$\n",
                "\n",
                "Donde:\n",
                "*   $\\theta$: Los parámetros (pesos).\n",
                "*   $\\eta$: La tasa de aprendizaje (Learning Rate).\n",
                "*   $\\nabla_{\\theta} J$: El gradiente de la función de costo respecto a los parámetros."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from ipywidgets import interact, FloatSlider, IntSlider\n",
                "\n",
                "# Configuración estética\n",
                "plt.style.use('ggplot')\n",
                "%matplotlib inline"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Visualización Interactiva del SGD\n",
                "Observemos cómo la tasa de aprendizaje afecta nuestra búsqueda del mínimo en una función de juguete: $f(x) = x^2$."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "fea14473669748129b305c1e3b2049bf",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "interactive(children=(FloatSlider(value=0.1, description='Learn Rate', max=1.1, min=0.01, step=0.05), IntSlide…"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/plain": [
                            "<function __main__.visualize_sgd(lr, steps)>"
                        ]
                    },
                    "execution_count": 2,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "def visualize_sgd(lr, steps):\n",
                "    x = np.linspace(-10, 10, 100)\n",
                "    y = x**2\n",
                "    \n",
                "    # Simulación de pasos\n",
                "    current_x = 9.0  # Empezamos en un punto lejano\n",
                "    history_x = [current_x]\n",
                "    history_y = [current_x**2]\n",
                "    \n",
                "    for _ in range(steps):\n",
                "        gradient = 2 * current_x # Derivada de x^2\n",
                "        current_x = current_x - lr * gradient\n",
                "        history_x.append(current_x)\n",
                "        history_y.append(current_x**2)\n",
                "        \n",
                "    plt.figure(figsize=(10, 6))\n",
                "    plt.plot(x, y, 'b-', alpha=0.5, label='Función de Pérdida $x^2$')\n",
                "    plt.plot(history_x, history_y, 'ro-', label='Pasos del SGD')\n",
                "    plt.title(f'SGD con Learning Rate: {lr}')\n",
                "    plt.xlabel('Peso (w)')\n",
                "    plt.ylabel('Pérdida (L)')\n",
                "    plt.legend()\n",
                "    plt.show()\n",
                "\n",
                "interact(visualize_sgd, \n",
                "         lr=FloatSlider(min=0.01, max=1.1, step=0.05, value=0.1, description='Learn Rate'), \n",
                "         steps=IntSlider(min=1, max=50, step=1, value=10, description='Pasos'))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Backpropagation: La Regla de la Cadena\n",
                "\n",
                "Si el SGD nos dice *cómo movernos*, el Backpropagation nos dice *cuánto contribuyó cada peso* al error final. Es simplemente una aplicación recursiva de la **Regla de la Cadena** de cálculo.\n",
                "\n",
                "Supongamos una red diminuta:\n",
                "$z = w \\cdot x + b\\\\$\n",
                "$a = \\sigma(z)\\\\$ (Función de activación)\n",
                "$\\\\L = (a - y)^2\\\\$ (Error cuadrático)\n",
                "\n",
                "Para ajustar $w$, necesitamos $\\frac{\\partial L}{\\partial w}$:\n",
                "$$\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial a} \\cdot \\frac{\\partial a}{\\partial z} \\cdot \\frac{\\partial z}{\\partial w}$$\n",
                "\n",
                "Esto es como desarmar una cebolla por capas, desde el resultado final hacia atrás."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Ejemplo de Juguete: Aprendiendo una suma simple\n",
                "Vamos a crear una neurona que aprenda que $2x = y$. El peso ideal debería ser 2."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "139baa10d7a54630a49cad83c61dd493",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "interactive(children=(IntSlider(value=100, description='epochs', max=500, min=10, step=10), FloatSlider(value=…"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/plain": [
                            "<function __main__.solve_toy_problem(epochs, lr)>"
                        ]
                    },
                    "execution_count": 3,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "def solve_toy_problem(epochs, lr):\n",
                "    # Datos: x=1, y=2; x=2, y=4; x=3, y=6\n",
                "    X = np.array([1, 2, 3, 4, 5])\n",
                "    Y = np.array([2, 4, 6, 8, 10])\n",
                "    \n",
                "    w = 0.0 # Inicialización\n",
                "    losses = []\n",
                "    weights = []\n",
                "    \n",
                "    for epoch in range(epochs):\n",
                "        # Elegimos un punto al azar (SGD puro)\n",
                "        idx = np.random.randint(0, len(X))\n",
                "        x, y_true = X[idx], Y[idx]\n",
                "        \n",
                "        # 1. Forward Pass\n",
                "        y_pred = w * x\n",
                "        \n",
                "        # 2. Computar Pérdida (MSE parcial)\n",
                "        loss = (y_pred - y_true)**2\n",
                "        losses.append(loss)\n",
                "        weights.append(w)\n",
                "        \n",
                "        # 3. Backpropagation (Gradiente)\n",
                "        # dL/dw = dL/dy_pred * dy_pred/dw\n",
                "        # dL/dy_pred = 2 * (y_pred - y_true)\n",
                "        # dy_pred/dw = x\n",
                "        grad = 2 * (y_pred - y_true) * x\n",
                "        \n",
                "        # 4. Actualizar peso\n",
                "        w = w - lr * grad\n",
                "        \n",
                "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
                "    ax1.plot(losses)\n",
                "    ax1.set_title('Pérdida por interacción')\n",
                "    ax1.set_xlabel('Iteración')\n",
                "    \n",
                "    ax2.plot(weights)\n",
                "    ax2.axhline(y=2.0, color='r', linestyle='--')\n",
                "    ax2.set_title(f'Evolución del Peso (Final: {w:.2f})')\n",
                "    ax2.set_xlabel('Iteración')\n",
                "    plt.show()\n",
                "\n",
                "interact(solve_toy_problem, \n",
                "         epochs=IntSlider(min=10, max=500, step=10, value=100), \n",
                "         lr=FloatSlider(min=0.001, max=0.1, step=0.005, value=0.01, readout_format='.3f'))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "### Conclusiones para llevar a casa:\n",
                "1.  **Backpropagation** es el algoritmo para calcular gradientes de forma eficiente.\n",
                "2.  **SGD** es la estrategia de optimización que usa esos gradientes para actualizar los pesos.\n",
                "3.  La **Tasa de Aprendizaje** es el hiperparámetro más crítico: muy alto y divergemos (saltamos el valle), muy bajo y nunca llegamos.\n",
                "\n",
                "¡Espero que esto les sea de gran utilidad en sus estudios! Quedo a su disposición para cualquier duda.\n",
                "\n",
                "Con cariño,\n",
                "**Su asistente IA** (con el apoyo del Dr. Fuentes)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
